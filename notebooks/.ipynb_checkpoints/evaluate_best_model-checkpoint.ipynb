{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "Evaluate performance of the best model\n",
    "* Train accuracy: 0.9237\n",
    "* Validation accuracy: 0.9025\n",
    "* Train loss: 0.2187\n",
    "* Validation loss: 0.3246"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use output of OpenPose with or without background\n",
    "BACKGROUND = False\n",
    "# Paths to videos for training\n",
    "PATHS = [\"/home/Datasets/RWF-2000\", f\"/home/Datasets/Openpose_RWF_blending\"]\n",
    "\n",
    "FRAME_FUNC = 'frame_diff'\n",
    "# To use frame diff to weight t (current) or t+1\n",
    "WEIGHT_CURRENT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 06:12:09.797533: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-27 06:12:09.870238: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-27 06:12:09.870286: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-27 06:12:09.870328: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-27 06:12:09.883052: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "Number of devices: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 06:12:13.678305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22493 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:1a:00.0, compute capability: 8.6\n",
      "2023-11-27 06:12:13.679243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22493 MB memory:  -> device: 1, name: NVIDIA RTX A5000, pci bus id: 0000:3e:00.0, compute capability: 8.6\n",
      "2023-11-27 06:12:13.680082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22493 MB memory:  -> device: 2, name: NVIDIA RTX A5000, pci bus id: 0000:89:00.0, compute capability: 8.6\n",
      "2023-11-27 06:12:13.680956: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22493 MB memory:  -> device: 3, name: NVIDIA RTX A5000, pci bus id: 0000:b1:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "!export TF_FORCE_GPU_ALLOW_GROWTH=True\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    tf.config.set_visible_devices(gpus[0:], 'GPU')\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "import random\n",
    "import numpy as np\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL_FRAMES_PER_VIDEO = 150\n",
    "FRAMES_PER_VIDEO = 50 + 1\n",
    "VIDEO_WIDTH, VIDEO_HEIGHT = 100, 100\n",
    "N_CHANNELS = 3\n",
    "\n",
    "def load_videos(video_IDs: list, video_frames: int = FRAMES_PER_VIDEO, video_width: int = VIDEO_WIDTH, video_height: int = VIDEO_HEIGHT,\n",
    "                video_channels: int = N_CHANNELS, dtype = np.float32, normalize: bool = False) -> tuple:\n",
    "    videos = np.empty((len(video_IDs), video_frames, video_height, video_width, video_channels), dtype=dtype)\n",
    "\n",
    "    # Indexes of frames to be kept to comply with video_frames\n",
    "    frames_idx = set(np.round(np.linspace(0, ORIGINAL_FRAMES_PER_VIDEO - 1, video_frames)).astype(int))\n",
    "\n",
    "    for i, video_ID in enumerate(video_IDs):\n",
    "        cap = cv2.VideoCapture(video_ID)\n",
    "        frames = []\n",
    "        index = 0\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if index in frames_idx:\n",
    "                frame = cv2.resize(frame, (video_width, video_height)).astype(dtype)\n",
    "                if normalize:\n",
    "                    frame /= 255.0\n",
    "                frames.append(frame)\n",
    "            index += 1\n",
    "        cap.release()\n",
    "\n",
    "        videos[i,] = np.array(frames)\n",
    "\n",
    "    return videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataGenerator class to load videos per batch, in case all videos do not fit in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, video_IDs: list, video_labels: dict, batch_size: int, paths: list = [''], video_width: int = VIDEO_WIDTH, video_height: int = VIDEO_HEIGHT,\n",
    "                video_frames: int = FRAMES_PER_VIDEO, video_channels: int = N_CHANNELS, dtype = np.float32, normalize: bool = False, shuffle: bool = True):\n",
    "        self.video_IDs = video_IDs\n",
    "        self.video_labels = video_labels\n",
    "        self.batch_size = batch_size\n",
    "        self.paths = paths\n",
    "        self.video_width = video_width\n",
    "        self.video_height = video_height\n",
    "        self.video_frames = video_frames\n",
    "        self.video_channels = video_channels\n",
    "        self.dtype = dtype\n",
    "        self.normalize = normalize\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_IDs) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_IDs = self.video_IDs[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        input_videos = []\n",
    "        \n",
    "        for index, path in enumerate(self.paths):\n",
    "            batch_IDs_full_path = [path+ID for ID in batch_IDs]\n",
    "\n",
    "            videos = load_videos(batch_IDs_full_path, self.video_frames, self.video_width, \n",
    "                                         self.video_height, self.video_channels, self.dtype, self.normalize)\n",
    "            \n",
    "            input_videos.append(videos)\n",
    "        \n",
    "        labels = np.array([self.video_labels[ID] for ID in batch_IDs])\n",
    "                    \n",
    "        return input_videos, labels\n",
    "            \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.video_IDs)\n",
    "        # Clear memory after epochs\n",
    "        gc.collect()\n",
    "        #K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Videos to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slug_from_path(path):\n",
    "    \"\"\"\n",
    "    Function to get slug from path\n",
    "    slug must contain /train or /val because there are repeated names\n",
    "    \"\"\"\n",
    "    # Try train index first\n",
    "    index = path.rfind('/train/')\n",
    "    if index == -1:\n",
    "        index = path.rfind('/test/')\n",
    "    return path[index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "test_video_IDs = [get_slug_from_path(path) for path in glob.glob(PATHS[0]+'/test/*/*')]\n",
    "\n",
    "test_video_labels = {video: 0 if 'NonFight' in video else 1 for video in test_video_IDs}\n",
    "\n",
    "test_generator = DataGenerator(test_video_IDs, test_video_labels, batch_size=10, paths=PATHS, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frame functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def tf_frame_diff(video):\n",
    "    return video[1:] - video[:-1]\n",
    "\n",
    "def tf_frame_dist(video):\n",
    "    video_diff = tf_frame_diff(video)\n",
    "    return K.sqrt(K.sum(K.square(video_diff), axis=-1, keepdims=True))\n",
    "\n",
    "if WEIGHT_CURRENT:\n",
    "    def tf_frame_diff_dist_combined(video):\n",
    "        video_diff = tf_frame_diff(video)\n",
    "        video_diff_current = tf.nn.relu(-video_diff)\n",
    "        video_diff_next = tf.nn.relu(video_diff)\n",
    "        video_diff_next_norm = K.sqrt(K.sum(K.square(video_diff_next), axis=-1, keepdims=True))\n",
    "        return K.concatenate([video_diff_current, video_diff_next_norm])\n",
    "else:\n",
    "    def tf_frame_diff_dist_combined(video):\n",
    "        video_diff = tf_frame_diff(video)\n",
    "        video_diff_current = tf.nn.relu(video_diff)\n",
    "        video_diff_prev = tf.nn.relu(-video_diff)\n",
    "        video_diff_prev_norm = K.sqrt(K.sum(K.square(video_diff_prev), axis=-1, keepdims=True))\n",
    "        return K.concatenate([video_diff_current, video_diff_prev_norm])\n",
    "    \n",
    "frame_func_dict = {'frame_diff': tf_frame_diff, 'frame_dist': tf_frame_dist, 'frame_diff_dist_combined': tf_frame_diff_dist_combined}\n",
    "frame_func = frame_func_dict[FRAME_FUNC]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pretrained model to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (0,) into shape (51,100,100,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m val_loss, val_acc \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mTest accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[7], line 29\u001b[0m, in \u001b[0;36mDataGenerator.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpaths):\n\u001b[1;32m     27\u001b[0m     batch_IDs_full_path \u001b[38;5;241m=\u001b[39m [path\u001b[38;5;241m+\u001b[39mID \u001b[38;5;28;01mfor\u001b[39;00m ID \u001b[38;5;129;01min\u001b[39;00m batch_IDs]\n\u001b[0;32m---> 29\u001b[0m     videos \u001b[38;5;241m=\u001b[39m \u001b[43mload_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_IDs_full_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideo_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideo_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                                 \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideo_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideo_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     input_videos\u001b[38;5;241m.\u001b[39mappend(videos)\n\u001b[1;32m     34\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_labels[ID] \u001b[38;5;28;01mfor\u001b[39;00m ID \u001b[38;5;129;01min\u001b[39;00m batch_IDs])\n",
      "Cell \u001b[0;32mIn[6], line 29\u001b[0m, in \u001b[0;36mload_videos\u001b[0;34m(video_IDs, video_frames, video_width, video_height, video_channels, dtype, normalize)\u001b[0m\n\u001b[1;32m     26\u001b[0m         index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     27\u001b[0m     cap\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m---> 29\u001b[0m     videos[i,] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(frames)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m videos\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (0,) into shape (51,100,100,3)"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model.evaluate(test_generator, verbose=2)\n",
    "print(f'Test loss: {val_loss:.4f}\\tTest accuracy: {val_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
